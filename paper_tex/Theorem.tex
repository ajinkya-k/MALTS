

\section{Robustness and Generalization of MALTS}
\label{sec:theory}
In this section we show that the MALTS framework estimates the correct distance metric and thus facilitates the correct estimates of CATEs. 
First, we define pairwise loss for $s_i$ and $s_l$ so that it is only finite for treatment-treatment or control-control matched pairs,
\begin{equation}
   loss[\mathcal{M},s_i,s_l] := \begin{cases} 
      e^{-\dis_\mathcal{M}(\x_i,\x_l)}|y_i-y_l| & \text{ if }t_i=t_l \\
      \infty & \text{otherwise}. \\
   \end{cases}
\end{equation}
Further, we define an empirical average loss over finite sample $\mathcal{S}_n$ of size $n$ as 
\begin{equation}
   L_{emp}(\mathcal{M},\mathcal{S}_n) := \frac{1}{n^2}\sum_{(s_i,s_l)\in(\mathcal{S}_n\times\mathcal{S}_n)} loss[\mathcal{M},s_i,s_l]
\end{equation}
and define an average loss over population $\mathcal{Z}$ as 
\begin{equation}
   L_{pop}(\mathcal{M},\mathcal{Z}) := \mathbb{E}_{z_i,z_l\overset{i.i.d}{\sim}\mu(\mathcal{Z})} \Big[ loss[\mathcal{M},z_i,z_l] \Big].
\end{equation}

Now, because the learned $\mathcal{M}(\mathcal{S}_{tr})$ on the set $\mathcal{S}_{tr}$ is the distance metric that minimizes the given objective function, we know that the following inequality is true, which states that the learned parameter has a lower training objective than that of the trivial parameter $\mathbf{0}$:
\begin{equation}
    \Big( c\|\mathcal{M}(\mathcal{S}_{tr})\|_\mathcal{F} + \Delta^{(C)}_{\mathcal{S}_{tr}}(\mathcal{M}(\mathcal{S}_{tr})) + \Delta^{(T)}_{\mathcal{S}_{tr}}(\mathcal{M}(\mathcal{S}_{tr})) \Big) \leq  \Big( c\|\mathbf{0}\|_\mathcal{F} + \Delta^{(C)}_{\mathcal{S}_{tr}}(\mathbf{0}) + \Delta^{(T)}_{\mathcal{S}_{tr}}(\mathbf{0}) \Big) =: g_0.
\end{equation}

Denoting the right hand side of the inequality by $g_0$ we note that we can limit our search space over distance metrics $\mathcal{M}$ that satisfy the following inequality:
\begin{equation}
    \|\mathcal{M}\|_\mathcal{F} \leq \frac{g_0}{c}.
\end{equation}
Thus, we observe that 
\begin{equation*}
    \Delta^{(C)}_{\mathcal{S}_{tr}}(\mathcal{M}) \leq \sum_{s_i\in\mathcal{S}^{(C)}_{tr}}\sum_{s_l \in \mathcal{S}^{(C)}_{tr}}\left| \frac{e^{-\dis_\mathcal{M}(\x_i,\x_l)}}{\sum_{s_k \in \mathcal{S}^{(C)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}(y_i-y_l)  \right| = \sum_{s_i\in\mathcal{S}^{(C)}_{tr}} \frac{\sum_{s_l \in \mathcal{S}^{(C)}_{tr}}loss[\mathcal{M},s_i,s_l]}{\sum_{s_k \in \mathcal{S}^{(C)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}.
\end{equation*}
We know that:
\begin{equation*}
    \forall i,k ~ \dis_\mathcal{M}(\x_i,\x_k) = (\x_i - \x_k)'\mathcal{M}(\x_i - \x_k) \leq \|\x_i - \x_k\|^2 \|\mathcal{M}\|_{\mathcal{F}} \leq \frac{g_0\mathbf{C}_x^2}{c}.
\end{equation*}
Together, the two previous lines imply:
\begin{equation}
\Delta^{(C)}_{\mathcal{S}_{tr}}(\mathcal{M}) 
\leq \frac{1}{n \exp{(-\frac{g_0\mathbf{C}_x^2}{c})}}\sum_{s_i\in\mathcal{S}^{(C)}_{tr}} \sum_{s_l \in \mathcal{S}^{(C)}_{tr}} loss[\mathcal{M},s_i,s_l] = \frac{n L_{emp}(\mathcal{M},\mathcal{S}^{(C)}_{tr})}{\exp{(-\frac{g_0\mathbf{C}_x^2}{c})}}.
\end{equation}
Similarly, 
\begin{equation}
\Delta^{(T)}_{\mathcal{S}_{tr}}(\mathcal{M}) \leq \frac{n L_{emp}(\mathcal{M},\mathcal{S}^{(T)}_{tr})}{\exp{(-\frac{g_0\mathbf{C}_x^2}{c})}}.
\end{equation}

Now, we define a few important concepts important for our results including covering number, multi-robustness and generalizability.
\begin{define}
(\textbf{Covering Number})
Let ($\mathcal{U},\rho$) be a metric space. Consider a subset $\mathcal{V}$ of $\mathcal{U}$, then $\hat{\mathcal{V}} \subset \mathcal{V}$ is called a $\gamma$-cover of $\mathcal{V}$ if for any $v \in \mathcal{V}$, we can always find a $\hat{v}\in\hat{\mathcal{V}}$ such that $\rho(v,\hat{v})\leq\gamma$. Further, the $\gamma$-covering-number of $\mathcal{V}$ under the distance metric $\rho$ is defined by
$\mathbf{N}(\gamma,\mathcal{V},\rho) := \min\big\{ |\hat{\mathcal{V}}| ~:~ \hat{\mathcal{V}} \text{ is a }\gamma\text{-cover of }\mathcal{V} \big\}$.
\end{define}
Note that $\mathbf{N}(\gamma,\mathcal{V},\rho)$ is finite if $\mathcal{U}$ is a compact.

% \begin{define}
% \textbf{(Smooth Distance Metric)} $\rho:\mathcal{X}\times\mathcal{X}\to\mathbb{R}^{+}$ is a smooth distance metric if there exists a monotonically increasing zero-intercept bounding function $\delta_\rho(\cdot)$ such that $\forall z_i,z_l \in \mathcal{Z}$ if $t_i=t_l$ and $\rho(\x_i,\x_l)\leq\epsilon$ then $|y_i-y_l|\leq\delta_\rho(\epsilon)$
% \end{define}

\begin{define}
(\textbf{Robustness})
\label{def:robust}
A learned distance metric $\mathcal{M}(\cdot)$ is $(K,\epsilon(\cdot))$-robust for a given $K$ and $\epsilon(\cdot):(\mathcal{Z}\times\mathcal{Z})^n \to \mathbb{R}$, if we can partition $\mathcal{X}$ into $K$ disjoint sets $\{C_i\}_{i=1}^K$ such that for all samples $\mathcal{S}_{tr}$ and the corresponding pair set $\mathcal{S}_{tr}^2 := \mathcal{S}_{tr} \times \mathcal{S}_{tr}$ associated to the sample $\mathcal{S}_{tr}$, we have for any pair of training units
$\big(s_1=(\x_1,y_1,t_1),s_2=(\x_2,y_2,t_2)\big)\in\mathcal{S}_{tr}^2$, and for any pair of units in the support $\big(z_1=(\x'_1,y'_1,t'_1),z_2=(\x'_2,y'_2,t'_2)\big)\in\mathcal{Z}^2,~\forall i,l \in \{1,...,K\}$, 
$$\text{if } \x_1,\x'_1 \in C_i \text{ and }  \x_2,\x'_2 \in C_l \text{ such that } t_1=t'_1=t_2=t'_2  \text{ then }$$ $$\Big|~ loss[\mathcal{M}(\mathcal{S}_{tr}),s_1,s_2] - loss[\mathcal{M}(\mathcal{S}_{tr}),z_1,z_2]~ \Big|\leq \epsilon(\mathcal{S}_{tr}).$$
\end{define}
Intuitively, \textit{robustness} means that for any possible units in the support, the loss is not far away from the loss of nearby units in training set, should some training units exist nearby. 
% As the training procedure aims at minimizing the cumulative loss, we can safely say that a robust method will not perform poorly out of sample.

\begin{define}
(\textbf{Multi-Robustness})
A learned distance metric $\mathcal{M}(\cdot)$ is $(K,\epsilon(\cdot))$-multirobust for a given $K$ and $\epsilon(\cdot):\mathcal{Z}^n \to \mathbb{R}$, if we can partition $\mathcal{X}$ into $K$ disjoint sets $\textbf{C} = \{C_i\}_{i=1}^K$ such that for all samples $\mathcal{S}_n$ and the corresponding pair set $\mathcal{S}_n^2 := \mathcal{S}_n \times \mathcal{S}_n$ associated to the sample $\mathcal{S}_n$, we have
$\forall\big(s_1=(x_1,y_1,t_1),s_2=(x_2,y_2,t_2)\big)\in\mathcal{S}_n^2,~\forall\big(z_1=(x'_1,y'_1,t'_1),z_2=(x'_2,y'_2,t'_2)\big)\in\mathcal{Z}^2,~\forall i,l \in \{1,...,K\}$ 

\begin{eqnarray*}
\left.\begin{aligned}
&
\mathrm{Given } \ \  \widehat{\overline{loss}}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] := \frac{1}{|C^{(t')}_i| |C^{(t')}_l|}\sum_{(s_i,s_l)\in C^{(t')}_i\times C^{(t')}_l} loss[\mathcal{M}(\mathcal{S}_n),s_1,s_2] & \\ 
&
\mathrm{ and } \ \  \overline{loss}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] := \mathbbm{E}[loss(\mathcal{M},Z_i,Z_l) ~|~ X_i^\prime\in C_i^{(t^\prime)}, X_l^\prime\in C_l^{(t^\prime)}]\\
& \forall C_i,C_l \in \textbf{C}, \ \ \Big|~ \widehat{\overline{loss}}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] - \overline{loss}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l]~ \Big|\leq \epsilon(\mathcal{S}_n). & 
\end{aligned}\right.
\end{eqnarray*}
\end{define} 

Intuitively, \textit{multi-robustness} means that for any possible pair of points from any possible pair of partition of $\mathcal{X}$, the empirical average loss over training points is not far away from the population average loss. As the training procedure aims at minimizing the cumulative loss, we can safely say that a multi-robust method will not perform poorly out of sample.

\begin{define}
(\textbf{Generalizability}) 
A learned distance metric $\mathcal{M}(\cdot)$ is said to generalize with respect to the given training sample $\mathcal{S}_n$ if 
$$ %\lim_{n\to\infty}
P_{\mathcal{S}_n}\left(\sum_{t'\in\mathcal{T}}\Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}^{(t')}_n) \Big| \geq \epsilon \right) \leq \delta_\epsilon$$
where $\delta_\epsilon$ is a decreasing function of $\epsilon$ with zero-intercept.
\end{define}

\begin{define}
(\textbf{Asymptotic Generalizability}) 
A learned distance metric $\mathcal{M}(\cdot)$ is said to asymptotically generalize with respect to the given training sample $\mathcal{S}_n$ if 
$$ \lim_{n\to\infty}\sum_{t'\in\mathcal{T}}\Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}^{(t')}_n) \Big| = 0$$
where $\delta_\epsilon$ is a decreasing function of $\epsilon$ with zero-intercept.
\end{define}

Given these definitions, we first show that the distance metric learned using \textsc{MALTS} is robust in Theorem~\ref{th: robust} and we extend the argument to show that it is also generalizable in Theorem~\ref{th: gen}. 

\begin{theorem}
\label{th: robust}
Given a fixed $\gamma$, $\beta$ and smooth distance metric $\|\cdot\|_2$ with bounding function $\delta(\cdot)$, with probability greater than $\left(1 - 2\exp\left(-\frac{2\beta^2(\rho^{(t')}_\gamma)^2}{B^2}\right)\right)$, the distance metric $\mathcal{M}(\cdot)$ learned using \textsc{MALTS} is: $${\Bigg(\mathbf{N}(\gamma,\mathcal{X},\|\cdot\|_2),\beta\Bigg)\mathrm{-multirobust}}.$$
\end{theorem}
\textbf{Proof. } Given $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\times\mathcal{T}$, we consider the following definition of a minimum sized $\gamma$-cover $\hat{\mathcal{V}}$ of the set $\mathcal{X}$ under the distance metrix $\|\cdot\|_2$: Partition the set into $K$ disjoint subsets $\textbf{C}_\gamma = \{C_i\}_{i=1}^{K}$ such that $K$ is the $\gamma$-covering-number of $\mathcal{X}$ under $\|\cdot\|_2$ (which is exactly equal to $|\hat{\mathcal{V}}|$) where each $C_i$ is contained in the $\gamma$-neighborhood of each $\hat{v}_i\in\hat{\mathcal{V}}$ and each $C_i$ contains at least one control and one treated sample. Note that if $\mathcal{X}$ is a compact convex set then such a cover and the corresponding packing $\textbf{C}_\gamma$ exists and $K=|\textbf{C}_\gamma|$ is finite. 

For any arbitrary $C_i$ and $C_l$ in $\textbf{C}_\gamma$ consider the empirical average loss for all training units $s_i \in C_i$ and $s_l \in C_l$ with treatment $t$ be 
\begin{eqnarray*}
\left.\begin{aligned}
& \widehat{\overline{loss}}\left[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l\right] = \frac{1}{|C^{(t')}_i\|C^{(t')}_l|}\sum_{(s_i,s_l)\in C^{(t')}_i\times C^{(t')}_l} loss[\mathcal{M}(\mathcal{S}_n),s_1,s_2] & 
\end{aligned}\right.
\end{eqnarray*}
and the expected loss for units $Z_i$ and $Z_l$ be
\begin{eqnarray*}
\left.\begin{aligned}
&
\overline{loss}\left[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l\right] = \mathbbm{E}\left[loss(\mathcal{M},Z_i,Z_l)~|~X_i^\prime\in C_i^{(t^\prime)}, X_l^\prime\in C_l^{(t^\prime)}\right]. &
\end{aligned}\right.
\end{eqnarray*}
% By Chebyshev's inequality, the probability that the empirical and population average are different by at least $\beta$ is 
% \begin{eqnarray*}
%     P\left(\left| \overline{loss}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] - \widehat{\overline{loss}}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] \right|\geq\beta\right)\leq \frac{\mathbbm{E}\left[loss^2(\mathcal{M}(\mathcal{S}_n),Z_i,Z_l) - \overline{loss}^2[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] \right]}{|C^{(t')}_i| |C^{(t')}_l|\beta^2} .
% \end{eqnarray*}
Let $B = \max_{z_1,z_2} loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)$, then by Hoeffding's inequality, the probability that the empirical and population average are different by at least $\beta$ is 
\begin{eqnarray*}
    P\left(\left| \overline{loss}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] - \widehat{\overline{loss}}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] \right|\geq\beta\right)\leq 2\exp\left(-\frac{2\beta^2|C^{(t')}_i| |C^{(t')}_l|}{B^2}\right).
\end{eqnarray*}

% We know that the R.H.S. of the above mentioned equation is less than $\frac{\mathbbm{E}\left[loss^2(\mathcal{M}(\mathcal{S}_n),Z_i,Z_l)] \right]}{|C^{(t')}_i| |C^{(t')}_l|\beta^2}$. We further expand the above equation by expanding the evaluation of loss based on the definition. 
% $$\frac{\mathbbm{E}\left[loss^2(\mathcal{M}(\mathcal{S}_n),Z_i,Z_l) - \overline{loss}^2[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] \right]}{|C^{(t')}_i| |C^{(t')}_l|\beta^2} \leq \frac{\mathbbm{E}\left[e^{-2\dis_{\mathcal{M}(\mathcal{S}_n)}(X'_i,X'_l)}|Y'_i - Y'_l|^2] \right]}{|C^{(t')}_i| |C^{(t')}_l|\beta^2}.$$ We know that the outcomes are bounded between with the range $[-\textbf{C}_y,\textbf{C}_y]$. Also, for all $a\geq0$, the value of $e^{-a} \leq 1$. Thus, $$\frac{\mathbbm{E}\left[e^{-2\dis_{\mathcal{M}(\mathcal{S}_n)}(X'_i,X'_l)}|Y'_i - Y'_l|^2] \right]}{|C^{(t')}_i| |C^{(t')}_l|\beta^2} \leq \frac{4\textbf{C}^2_y}{|C^{(t')}_i| |C^{(t')}_l|\beta^2}.$$
Let's define $\rho^{(t')}_{\gamma}$, the density of the $\gamma$-cover for treatment $t'$, as the number of units with treatment $t'$ in the smallest partition set. 
$$\rho^{(t')}_{\gamma} = \min_i |C^{(t')}_i|.$$
Hence, we can further simplify the inequality as 
$$P\left(\left| \overline{loss}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] - \widehat{\overline{loss}}[\mathcal{M}(\mathcal{S}_n),C^{(t')}_i,C^{(t')}_l] \right|\geq\beta\right) \leq  2\exp\left(-\frac{2\beta^2(\rho^{(t')}_\gamma)^2}{B^2}\right).
% \frac{4\textbf{C}^2_y}{(\rho^{(t')}_\gamma)^2
% \beta^2}. 
$$

\begin{lemma}
\label{lm: whpavgloss}
Given training sample $\mathcal{S}_{tr}\overset{i.i.d}{\sim}\mu(\mathcal{Z})$ where $n_{tr}^{(t')}$ is the number of units with $t_i=t'$ in $\mathcal{S}_{tr}$, and choosing $B>0$ for which $loss[\cdot,z_i,z_l]\leq B$ $\forall z_i,z_l\in\mathcal{Z}$ (B exists because $\mathcal{X}$ is compact and $\mathcal{Y}$ is bounded): if a learning algorithm $\mathcal{A}(\mathcal{S}_{tr})$ is $(K,\epsilon(\cdot))$-multirobust with the probability $p_{mr}(\epsilon)$ then for any $\mathcal{E}>0$, with probability greater than or equal to $(1-\mathcal{E})(p_{mr}(\epsilon))^{K^2}$ we have
$$\forall t'\in\mathcal{T},~\Big| L_{pop}(\mathcal{A}(\mathcal{S}_{tr}),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{A}(\mathcal{S}_{tr}),\mathcal{S}_{tr}^{(t')}) \Big| \leq \epsilon(\mathcal{S}^{(t')}_n) + 2B\sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n_{tr}^{(t')}}}~.$$
\end{lemma}

\begin{theorem}
\label{th: gen}
The distance metric $\mathcal{M}(\cdot)$ learned using \textsc{MALTS} is generalizable and asymptotically generalizable. 
\begin{enumerate}
    \item Generalizability 
    \begin{eqnarray*}
     \lefteqn{P_{\mathcal{S}_n}\Bigg(
     \begin{tabular}{c} 
     $\sum_{t'\in\mathcal{T}} \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big|$ \\
     $\leq
     2|\mathcal{T}|\beta+ \sum_{t'\in\mathcal{T}} 2B\sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n^{(t')}}}$
    \end{tabular}
     \Bigg)}\\
     &&\hspace*{150pt}\geq (1-\mathcal{E})^{\mathcal{T}}\left(1-2\exp\left(-\frac{2\beta^2(\rho_\gamma)^2}{B^2}\right)\right)^{|\mathcal{T}|K^2}.
\end{eqnarray*}
\item Asymptotic Generalizability$$ \hspace*{-25pt}\lim_{n\to\infty} \Bigg( \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(C)}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}^{(C)}_n) \Big| + \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(T)}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}^{(T)}_n) \Big| \Bigg) = 0$$
\end{enumerate}
\end{theorem}
\textbf{Proof (Theorem~\ref{th: gen})}. By Theorem~\ref{th: robust}, we know that the distance metric $ \mathcal{M}(\cdot) $ learned using \textsc{MALTS} is $(\mathbf{N}(\gamma,\mathcal{X},\|\cdot\|_2),\beta)$-multirobust with probability more than $1-2\exp\left(-\frac{2\beta^2(\rho^{(t')}_\gamma)^2}{B^2}\right)$.
Also, inferring from Lemma~\ref{lm: whpavgloss}, for any arbitrary $\forall t' \in \mathcal{T}$ and $\mathcal{E}>0$ we have
\begin{eqnarray*}
    P_{\mathcal{S}_n}\Bigg( \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big| \leq \beta + 2B\sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n^{(t')}}} \Bigg) \\ 
    \geq (1-\mathcal{E})\left(1-2\exp\left(-\frac{2\beta^2(\rho^{(t')}_\gamma)^2}{B^2}\right)\right)^{K^2}.
\end{eqnarray*}

Let $\rho_\gamma = \min_{t'} \rho^{(t')}_\gamma$. Then, summing over all possible $t'\in\mathcal{T}$ we have:
\begin{eqnarray*}
     \lefteqn{P_{\mathcal{S}_n}\Bigg(
     \begin{tabular}{c} 
     $\sum_{t'\in\mathcal{T}} \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big|$ \\
     $\leq
     2|\mathcal{T}|\beta+ \sum_{t'\in\mathcal{T}} 2B\sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n^{(t')}}}$
    \end{tabular}
     \Bigg)} \\
     &&\hspace*{150pt}\geq (1-\mathcal{E})^{\mathcal{T}}\left(1-2\exp\left(-\frac{2\beta^2(\rho_\gamma)^2}{B^2}\right)\right)^{|\mathcal{T}|K^2}.
\end{eqnarray*}
$\gamma$ in Theorem~\ref{th: robust} was arbitrary, allowing us to take it to 0 in such a way that $K$ increases at a rate smaller than $\min_{t'} n_{tr}^{(t')}$ increases and $\rho_\gamma$ strictly monotonically increases to $\infty$ as $n_{tr}$ approaches $\infty$. Thus we can reduce $\beta$ to $0$ at the rate slower than $\frac{1}{\rho^2_\gamma}$. $\mathcal{E}$ was also set arbitrarily, allowing us to take it to 0 slowly enough such that as $n_{tr}\to\infty$, each of the $n_{tr} ^{(t')}\to\infty$ we have: 
\begin{equation}
    \lim_{n_{tr}\to\infty} \Bigg( \sum_{t'\in\mathcal{T}} \Big| L_{pop}(\mathcal{M}(\mathcal{S}_{tr}),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_{tr}),\mathcal{S}^{(t')}_{tr}) \Big| \Bigg) = 0.
\end{equation}

Now that we have theoretical proved the functionality of MALTS, we will next discuss and compare MALTS performance with other methods on different datasets.
