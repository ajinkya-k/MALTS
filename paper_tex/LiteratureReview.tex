% !TEX root = main.tex
\section{Related work}

Since the 1970's, the causal inference literature on matching methods has been concentrated on dimension reduction techniques \citep[e.g.,][]{rubin1973matching,rubin1973use,rubin1976multivariate,cochran1973controlling}. In this literature, the leading approach for dimension reduction uses the propensity score, the conditional probability of treatment given covariate information.
Propensity score methods target average treatment effects and so do not produce exact matches or almost-exact matches. When treatment is binary, they project data onto one dimension, and closeness of units in propensity score does not imply their closeness in covariate space. As a result, the matches cannot directly be used for estimating heterogeneous treatment effects. Fast and flexible matching methods have been studied in the literature to estimate average treatment effects because fully efficient matching might be computationally inefficient \citep{gu1993, imbens2004nonparametric}. However, these method are often ineffective in producing high quality CATE estimates. Regression methods can be used for CATE estimation, but this assumes that the regression method is correctly specified -- or in the case of doubly robust estimation \citep[e.g., ][]{farrell2015robust}, either the propensity model or the outcome model needs to be correctly specified. 
Machine learning approaches generalize regression approaches and can create models that are extremely flexible and predict outcomes accurately for both treatment and control groups \citep{hill2011bayesian,chernozhukov2016double,hahn2017bayesian}. However, complicated regression methods lose the interpretability inherent to almost-exact matches.
%Analogously,  again leading to a sacrifice in interpretability.
 In practice, MALTS performs similarly to (or better than) several machine learning methods in our experiments, despite being restricted to interpretable almost-exact matches with an interpretable distance metric.

A flexible setup for producing high-quality matches is provided by the optimal matching literature \citep{rosenbaum2016imposing}. These are built on network flow algorithms and integer programming to produce matches that are constrained in user-defined ways \citep{zubizarreta2012using,zubizarreta2014matching,keele2014optimal,resa2016evaluation,AlamRu15,AlamRu15nonparam,pmlr-v54-kallus17a}. In all of these approaches, the user defines the distance metric rather than learning it through data, which is time-consuming and likely inaccurate, potentially leading to poor quality of the matched groups. 

An alternative to optimal matching is coarsened exact matching (CEM) \citep{iacus2012causal}, an approach that requires users to specify explicit bins for all covariates on which to construct matches. 
This requires users to know in advance that the outcomes are insensitive to movements within many high-dimensional bins, 
which is essentially equivalent to the user knowing the answer to the problem we investigate in this work. Large amounts of user choice to define these bins can also lead to unintentional user bias. By \textit{learning} the stretching rather than asking the user to define it as in CEM, this bias is potentially reduced. 
Alternatively, \cite{zhao2004} and \cite{imbens2004nonparametric} discusses the choice of distance metric for matching. The approach by \cite{zhao2004} depends on the correlations between treatment choice, outcome and covariates. However, this approach assumes a model for the relationship between the outcome and covariates, or the treatment choice and covariates. Hence, under model misspecification, the estimator may not be consistent. MALTS learns a distance metric without any model assumptions. 
The present work builds on work of  \cite{wang2017flame, DiengEtAl2018} where a discrete distance metric is learned by considering the prediction quality of the covariate sets. 

There is substantial work on learning distance metrics \citep[e.g.,][]{goldberger2005neighbourhood,weinberger2006distance,weinberger2009distance}, where the goal is to learn a distance metric in latent space to separate different classes of data in supervised learning, often with a margin. This is different from our goal of matching for causal inference, but some of our proofs were inspired by this work in supervised learning. 

MALTS was used for the Atlantic Causal Inference Competition \citep{harsh2019acic}.