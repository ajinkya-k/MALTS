%!TEX root = main.tex
\section{Introduction}
Matching methods are used throughout the social and health sciences to make causal conclusions where access to randomized trials is scarce but observational data are widely available. 
Matching methods construct groups of similar individuals, some of whom select into treatment and some of whom select into control, allowing for direct comparison of outcomes between these populations. Matching methods are particularly interpretable since they allow fine-grained troubleshooting of the data. 
For instance, examining the matched groups may allow the user to detect unmeasured confounding that led some units to have a higher chance of being treated or a higher chance of leading to a positive outcome.
Having high-quality matches also allows the user to estimate nonlinear treatment effects with lower bias than parametric approaches.
The quality of the matches is our main consideration in this work.

Typically, matching methods place units that are close together into the same matched group, where closeness is measured in terms of a pre-defined distance (e.g., exact, coarsened exact, Euclidean, etc.), while maintaining balance constraints between treatment and control units. Despite its merits, this classical paradigm has flaws, namely that it relies heavily on a prespecified distance metric. The distance metric cannot be determined without an understanding of the importance of the variables;
for instance, the quality of matches for any prespecified distance weighing all covariates equally will degrade as the number of irrelevant covariates increases. 
This is true irrespective of the matching methodology employed.
This has previously been referred to as the toenail problem \citep{wang2017flame, DiengEtAl2018}, where the inclusion of irrelevant covariates (like toenail length) with nonzero weights can overwhelm the metric for matching. A related concern is that the covariates may be scaled differently, where a given distance along one covariate has a different impact than the same distance along a different covariate; in this case, if the weights on the covariates are chosen poorly, the total distance metric can inadvertently be determined by less relevant covariates, again leading to lower quality matches.

Ideally, the distance metric would capture important covariates that significantly contribute in generating the outcome, so that after matching, treatment effect estimates computed within the matched groups would be accurate estimates of treatment effects. If the researcher knows how to choose the distance metric so that it yields accurate treatment effect estimates, this would solve the problem. However, there is no reason to believe that this is achievable in high-dimensional and complex data settings. Producing high dimensional functions to characterize data is a task at which humans are not naturally adept.

In this work, we propose a framework for matching where an interpretable distance measure between matched units is learned from a held-out training set. As long as the distance metric generalizes from the training set to the full sample, we are able to compute high-quality matches and accurate estimates of conditional average treatment effects (CATEs) within the matched groups. One can use any form of distance metric to train, and in this work, we focus on exact matching for discrete variables and generalized Mahalanobis distances for continuous variables. By definition, the generalized Mahalanobis distance is determined by a matrix. If the matrix is diagonal, the distance calculation represents a stretch for each covariate. Irrelevant covariates will be compressed so that their values are always effectively zero. Highly relevant covariates will be stretched so that for two units to be considered a match, they must have very similar values for those covariates. In this way, diagonal matrices lead to very interpretable distance metrics. If the Mahalanobis distance matrix is not constrained to be diagonal, then it induces a stretch and rotation, leading to more flexible but less interpretable notions of distance. 

The new framework is called Learning-to-Match, and the algorithm introduced in this work is called Matching After Learning to Stretch (MALTS). We tested MALTS against several other matching methods in simulation studies, where ground truth CATEs are known. In these experiments, MALTS achieves substantially and consistently better results than other matching methods including Genmatch, propensity score matching, and standard (non-learned) Mahalanobis distance in estimating CATEs. Even though our method is heavily constrained to produce interpretable matches, it performs at the same level as non-matching methods that are designed to fit extremely flexible but uninterpretable models directly to the response surface.

%We use MALTS to study the effect of a large drug treatment program (Breaking the Cycle - BTC  \cite{}) on crime reduction, which is relevant to the current opioid epidemic. We find that the important covariates for matching in the BTC drug treatment program are ???, and we identify subgroups for which there is a (heterogeneous) treatment effect.