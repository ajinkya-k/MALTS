% !TEX root = main.tex
\section*{Appendix A}
In this section we provide proofs for theorems and lemmas discussed in Section~\ref{sec:theory}.\\\\
\allowdisplaybreaks
% \textbf{Proof (Theorem~\ref{th: robust})}. Given $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\times\mathcal{T}$, we consider the following definition of a minimum sized $\gamma$-cover $\hat{\mathcal{V}}$ of the set $\mathcal{X}$ under the distance metrix $\|\cdot\|_2$: Partition the set into $K$ disjoint subsets $\{C_i\}_{i=1}^{K}$ such that $K$ is the $\gamma$-covering-number of $\mathcal{X}$ under $\|\cdot\|_2$ (which is exactly equal to $|\hat{\mathcal{V}}|$) where each $C_i$ is the $\gamma$-neighborhood of each $\hat{v}_i\in\hat{\mathcal{V}}$ and each $C_i$ contains at least one control and one treated sample. Note that if such a cover exists, then since $\mathcal{X}$ is a compact convex set, $K$ is finite. 

% We further assume that distance metric $\|\cdot\|_2$ is a smooth distance metric with bounding function $\delta(\cdot)$. This implies that $\delta(\cdot)$ is a monotonically increasing zero-intercept function such that $\forall z_1,z_2 \in \mathcal{Z}$ if $t_1=t_2$ and $\|\x_1-\x_2\|_2\leq\epsilon$ then $|y_1-y_2|\leq\delta(\epsilon)$. This further implies that for any $z_1$ and $z_2$ such that $x_1, x_2 \in C_{i}$ and $t_1=t_2$ then, since they are both in the same ball of radius $\gamma$, we have $|y_1-y_2|\leq\delta(\gamma)$.

% For some $s_1=(\x_1,y_1,t_1)$ and $s_2=(\x_2,y_2,t_2)$ in the training set $\mathcal{S}_n$ and $z_1=(\x'_1,y'_1,t'_1)$ and $z_2=(\x'_2,y'_2,t'_2)$ in $\mathcal{Z}$ such that $\x_1,\x'_1\in C_{i}$, $\x_2,\x'_2\in C_{l}$, and $t_1=t'_1=t_2=t'_2$, then we try to bound the following quantity: $$\Big|loss[\mathcal{M}(\mathcal{S}_n),s_1,s_2]-loss[\mathcal{M}(\mathcal{S}_n),z_1,z_2]\Big| = \Big| |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} (y_1-y_2)| - |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y'_1 - y'_2)| \Big|.$$
% From the reverse triangle inequality we know
% \begin{eqnarray*}
%   \lefteqn{
%   \Big| |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} (y_1-y_2)| - |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y'_1 - y'_2)| \Big|
%   }\\
%   &\leq&
%   \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} (y_1-y_2) - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y'_1 - y'_2) \Big|\\
%   &=&
%           \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} y_1 - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} y_1 +   e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} y_1 
%   - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} y'_1 
%   \\&&- e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} y_2 + e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} y_2 - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} y_2
%   + e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} y'_2 \Big|\\
%   & =&
%   \Bigg| y_1 \Big( e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)}  - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} \Big) +   e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y_1 - y'_1)
%   \\&&- y_2 \Big( e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)}  - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} \Big) - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y_2 - y'_2) \Bigg|,
% \end{eqnarray*}
% and applying the triangle inequality, 
% \begin{eqnarray*}
%   &\leq&
%   \Big| y_1 \Big( e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)}  - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} \Big)\Big| +  \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y_1 - y'_1) \Big|
%   \\
%   &&+\Big| y_2 \Big( e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} -  e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} \Big) \Big| + \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y_2 - y'_2) \Big|.
% \end{eqnarray*}
% For any $y\in \mathcal{Y}$, we know that $|y|\leq\mathbf{C}_y$. Thus,
% \begin{eqnarray*}
%   \lefteqn{
%   \Big| |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} (y_1-y_2)| - |e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} (y'_1 - y'_2)| \Big|
%   }\\
%   &\leq&
%   2\mathbf{C}_y \Bigg( \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)}  - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)}\Big|\Bigg) +  |y_1-y'_1| + |y_2-y'_2|.
% \end{eqnarray*}
% By the smoothness of distance metric $\|\cdot\|_2$ and the fact that the two points are in the same $\gamma$-sized ball, we know that $|y_1-y'_1| + |y_2-y'_2|\leq 2\delta(\gamma)$. Hence,
% \begin{eqnarray*}
% \lefteqn{\Big|loss[\mathcal{M}(\mathcal{S}_n),s_1,s_2]-loss[\mathcal{M}(\mathcal{S}_n),z_1,z_2]\Big|}\\
%   &\leq&
%   2\Bigg( \mathbf{C}_y \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)}  - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)}\Big| +  \delta(\gamma) \Bigg).
% \end{eqnarray*}
% If we multiply the right-hand-side of the inequality with a number greater than 1, then the inequality will not change. Hence,
% \begin{eqnarray*}
%      \lefteqn{
%      2\Bigg(\mathbf{C}_y \Big| e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} - e^{-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x'_1,\x'_2)} \Big| + \delta(\gamma) \Bigg)}\\
%      &\leq&
%      2\Bigg(\mathbf{C}_y  \Big| e^{\dis_{\mathcal{M}(\mathcal{S}_n)}(z_1,z_2)-\dis_{\mathcal{M}(\mathcal{S}_n)}(\x_1,\x_2)} - 1 \Big| +  \delta(\gamma) \Bigg)\\
% %\end{equation}
% %\begin{equation}
%      &=&
%      2\Bigg(\mathbf{C}_y \Big| e^{(\x'_1-\x'_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1-\x'_2)-(\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x_1-\x_2)} - 1 \Big| + \delta(\gamma) \Bigg)\\
%      &=&
%      2\Bigg(\mathbf{C}_y \Big| exp\Bigg((\x'_1-\x'_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1-\x'_2) - (\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1-\x'_2)
%      \\&&+ (\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1-\x'_2) -(\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x_1-\x_2)\Bigg) - 1 \Big| + \delta(\gamma) \Bigg) \Bigg)\\
%      &=&
%      2\Bigg(\mathbf{C}_y \Big| exp\Bigg((\x'_1-\x'_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1 - \x'_2 - \x_1 + \x_2)
%      \\&&+ (\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1 - \x'_2 - \x_1 + \x_2)\Bigg) - 1 \Big| + \delta(\gamma) \Bigg) \Bigg)\\
%      &=&
%      2\Bigg(\mathbf{C}_y \Big| exp\Bigg((\x'_1-\x'_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1 - \x_1) + (\x'_1-\x'_2)^T\mathcal{M}(\mathcal{S}_n)(\x_2 - \x'_2)
%      \\&&+ (\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x'_1 - \x_1) + (\x_1-\x_2)^T\mathcal{M}(\mathcal{S}_n)(\x_2 - \x'_2)\Bigg) - 1 \Big| + \delta(\gamma) \Bigg) \Bigg)\\
%      &\leq& 
%      2\Bigg( \mathbf{C}_y \Bigg| \exp\Big( \|\x_1-\x_2\|_2\|\mathcal{M}(\mathcal{S}_n)\|_\mathcal{F}\|\x_1-\x'_1\|_2 
%      + \|\x_1-\x_2\|_2\|\mathcal{M}(\mathcal{S}_n)\|_\mathcal{F}\|\x_2-\x'_2\|_2 
%      \\&&+ \|\x'_1-\x'_2\|_2\|\mathcal{M}(\mathcal{S}_n)\|_\mathcal{F}\|\x_1-\x'_1\|_2
%      + \|\x'_1-\x'_2\|_2\|\mathcal{M}(\mathcal{S}_n)\|_\mathcal{F}\|\x_2-\x'_2\|_2 
%      \Big)
%       - 1 \Bigg| +  \delta(\gamma) \Bigg)\\
% %\end{split}
% %\end{equation}
% %\begin{equation}
%     &\leq& 2 \Bigg( \mathbf{C}_y  \Big| e^{4 \mathbf{C}_x \gamma \|\mathcal{M}(\mathcal{S}_n)\|_\mathcal{F} } - 1 \Big| + \delta(\gamma) \Bigg)
%     = 
%     2\mathbf{C}_y \Bigg( \Big| e^{4 \mathbf{C}_x \gamma g_0/c } - 1 \Big| \Bigg) +  2\delta(\gamma).
% \end{eqnarray*}
% Hence, we conclude that our fixed $\gamma$, the distance metric learned using \textsc{MALTS} algorithm is robust by the definition of robustness.

%---------------------------------------------

% proof for lemma 1

\textbf{Proof (Lemma~\ref{lm: whpavgloss})}. If $(D_1,\dots,D_K)$ is the multinomially distributed random vector with parameters $d$ and $p_1, \dots, p_K$ then, by the Bretagnolle-Huber-Carol inequality, $ P(\sum_{i=1}^{K} \Big| \frac{D_i}{d} - p_i \Big|\geq\lambda)\leq 2^K e^{-\frac{d\lambda^2}{2}}$. Thus, for our case, we can consider $N_i$ corresponding to the set of indices of units in sample $\mathcal{S}^{(t')}_n$ such that their $x$'s are contained in the partition $\mathbf{C}_{i}$ as in Theorem~\ref{th: robust}. Hence, by the Bretagnolle-Huber-Carol inequality, we know that 
$$
    P\Bigg(~ \sum_{i=1}^{K} \Big| \frac{|N_i|}{n^{(t')}} - \mu(\mathbf{C}_{i}) \Big| \geq \sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n^{(t')}}} ~\Bigg) \leq \mathcal{E}~.
$$
Now, for some arbitrary $t'\in\mathcal{T}$ let us consider $\Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big|$. 
We know that
{\allowdisplaybreaks
\begin{eqnarray*}
%   \begin{split}
\lefteqn{
        \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big| }\\
       & =& 
        \Bigg| \sum_{i,j=1}^{K} \Big( \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1=(\x'_1,y'_1,t'_1),z_2=(\x'_2,y'_2,t'_2))~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~\mu(\mathbf{C}_i)\mu(\mathbf{C}_j) \Big)
        \\&&- 
        \frac{1}{(n^{(t')})^2}\sum_{s_1,s_2\in\mathcal{S}^{(t')}_n} loss(\mathcal{M}(\mathcal{S}_n),s_1,s_2)
        \Bigg|\\
        & =& 
        \Bigg| \sum_{i,j=1}^{K} \left( \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~\mu(\mathbf{C}_i)\mu(\mathbf{C}_j) \right) \\&& - \sum_{i,j=1}^{K} \left(\mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~\mu(\mathbf{C}_i)~\frac{|N_j|}{n^{(t')}}\right)
        \\&&
        + \sum_{i,j=1}^{K} \left(\mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~\mu(\mathbf{C}_i)~\frac{|N_j|}{n^{(t')}}\right) \\&&+ \sum_{i,j=1}^{K} \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~
        \frac{|N_i|}{n^{(t')}} \frac{|N_j|}{n^{(t')}} \\&&- \sum_{i,j=1}^{K} \mathbb{E}_{\x'_1,\x'_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~
        \frac{|N_i|}{n^{(t')}} \frac{|N_j|}{n^{(t')}}
        \\&&- 
        \frac{1}{(n^{(t')})^2}\sum_{s_1,s_2\in\mathcal{S}^{(t')}_n} loss(\mathcal{M}(\mathcal{S}_n),s_1,s_2)
        \Bigg|\\
        &\leq& \Bigg| \sum_{i,j=1}^{K} \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~\mu(\mathbf{C}_i)\Big(\mu(\mathbf{C}_j) - \frac{|N_j|}{n^{(t')}}\Big) \Bigg|\\
        &&+\Bigg| \sum_{i,j=1}^{K} \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~
        \frac{|N_j|}{n^{(t')}}
        \Big(\mu(\mathbf{C}_i) - \frac{|N_i|}{n^{(t')}}\Big) \Bigg|\\
       && + \Bigg| \sum_{i,j=1}^{K} \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~
        \frac{|N_i|}{n^{(t')}} \frac{|N_j|}{n^{(t')}} \\&& - \frac{1}{(n^{(t')})^2}\sum_{s_1,s_2\in\mathcal{S}^{(t')}_n} loss(\mathcal{M}(\mathcal{S}_n),s_1,s_2) \Bigg|\\
    &\leq& 2B\sum_{i=1}^{K}\Big| \frac{|N_i|}{n^{(t')}} - \mu(\mathbf{C}_i) \Big| + \Bigg| \sum_{i,j=1}^{K} \mathbb{E}_{z_1,z_2}[loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2)~|~\x'_1\in\mathbf{C}_i,\x'_2\in\mathbf{C}_j]~
        \frac{|N_i|}{n^{(t')}} \frac{|N_j|}{n^{(t')}} \\&& - \frac{1}{(n^{(t')})^2}\sum_{s_1,s_2\in\mathcal{S}^{(t')}_n} loss(\mathcal{M}(\mathcal{S}_n),s_1,s_2) \Bigg| \text{ where $B$ is }\max_{z_1,z_2} loss(\mathcal{M}(\mathcal{S}_n),z_1,z_2).
\end{eqnarray*}
}
Hence, we can conclude for all $t'\in\mathcal{T}$ we have
\begin{eqnarray*}
   & P_{\mathcal{S}_n}\Bigg( \Big| L_{pop}(\mathcal{M}(\mathcal{S}_n),\mathcal{Z}^{(t')}) - L_{emp}(\mathcal{M}(\mathcal{S}_n),\mathcal{S}_n^{(t')}) \Big| \geq \epsilon(\mathcal{S}^{(t')}_n) + 2B\sqrt{\frac{2K~\ln(2)~+~2~\ln(1/\mathcal{E})}{n^{(t')}}} \Bigg) \\
    &\leq 1 - (1-\mathcal{E})(p_{mr}(\epsilon))^{K^2}~.
\end{eqnarray*}
For notational simplicity, we do not consider ties here, though in our code, ties are handled by random selection.

%-------------------------------------------------

\begin{lemma}
\label{lm: smoothy}
Given a smooth distance metric $\mathcal{M}$ and treatment choice variable $t'\in\mathcal{T}$, if we estimate the counterfactual $\hat{y}^{(t')}(\x)$ for any given $z = (\x,y,t) \in \mathcal{Z}$  by nearest neighbor matching on a finite sample $\mathcal{S}_n \overset{i.i.d}{\sim}\mu(\mathcal{Z}^n)$ using distance metric $\mathcal{M}$, then the estimated counterfactual $\hat{y}^{(t')}(\x)$ and the true counterfactual $y^{(t')}(\x)$ are farther than $\epsilon$ with probability less than $\delta(\epsilon,\mathcal{M},n)$,
$$ P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{y}^{(t')}(\x) - y^{(t')}(\x)| \geq \epsilon \Big) \leq \delta(\epsilon,\mathcal{M},n). $$
\end{lemma}

%-------------------------------------------------

\textbf{Proof (Lemma~\ref{lm: smoothy})}.
Let $\mathcal{B}_{\mathcal{M}}(\x,r)$ be an open ball of radius $r>0$ under distance metric $\mathcal{M}$, centered around point a fixed point $\x \in \mathcal{X}$. We know that there is a nonzero probability mass around any point $\x \in \mathcal{X}$,  
\begin{equation}
    \forall r>0, P_{X \sim \mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))>0.
\end{equation}

As $\mathcal{S}_n = \{Z_1=(X_1,Y_1,T_1),
\dots, Z_n=(X_n,Y_n,T_n)\} \overset{i.i.d}{\sim}\mu(\mathcal{Z})$, the probability that no unit $Z_i$ with $T_i=t'$ from a $n$-sized random sample $\mathcal{S}_n=\{Z_i\}^n_{i=1}$ lies within the $r$-neighborhood of a given unit $z=(\x,y,t) \in \mathcal{Z}$ is
\begin{equation}
    \label{eq: notinball}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n) }\Bigg(
    \bigwedge_{Z_i\in\mathcal{S}_n} \Big(X_i \notin \mathcal{B}_{\mathcal{M}}(\x,r) \land T_i = t'\Big) \Bigg)
    \leq 
     P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n) }\Bigg(
    \bigwedge_{Z_i\in\mathcal{S}_n} \Big(X_i \notin \mathcal{B}_{\mathcal{M}}(\x,r) \Big) \Bigg)
    % =
    % \Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(x,r))\Big)^{n} .
\end{equation}

\begin{equation}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n) }\Bigg(
    \bigwedge_{Z_i\in\mathcal{S}_n} \Big(X_i \notin \mathcal{B}_{\mathcal{M}}(\x,r) \Big) \Bigg)
    =
    \Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))\Big)^{n} .
\end{equation}

From Equation~\ref{eq: notinball}, we can deduce that the probability that every unit with $T_i=t'$ in randomly drawn sample $\mathcal{S}_n$ is at least at a distance $r$ from a given $z=(\x,y,t)$ is 
\begin{equation}
\label{eq: notnearr}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big(\min_{\substack{Z_i\in\mathcal{S}_n\\T_i=t'}} \dis_{\mathcal{M}}(X_i,\x) \geq r \Big) 
    \leq
    \Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))\Big)^{n}.
\end{equation}
We infer from Equation~\ref{eq: notnearr} that
\begin{equation}
\label{eq: notnearrnn}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big(\dis_{\mathcal{M}}(1NN^{\mathcal{S}_n}_{\mathcal{M}}(\x,t'),\x) \geq r \Big) 
    \leq 
    \Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))\Big)^{n}.
\end{equation}

Combining the smoothness of distance metric $\dis_\mathcal{M}$, the counterfactual estimation $\hat{y}^{(t')}(\x)$ = $y\Big(1NN^{\mathcal{S}_n}_{\mathcal{M}}(\x,t')\Big)$ and Equation~\ref{eq: notnearrnn}, we infer that for some $\epsilon_r$ corresponding any $r>0$ we have:
\begin{eqnarray*}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{y}^{(t')}(\x) - y^{(t')}(\x)| \leq \epsilon_r \Big) \geq 
    \left(1-\beta_{\dis_\mathcal{M}}\right)\left(1-\Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))\Big)^{n}\right).
\end{eqnarray*}
\begin{eqnarray*}
    P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{y}^{(t')}(\x) - y^{(t')}(\x)| \geq \epsilon_r \Big) \leq 1 -
    \left(1-\beta_{\dis_\mathcal{M}}\right)\left(1-\Big(1 - P_{X\sim\mu(\mathcal{X})}(X \in \mathcal{B}_{\mathcal{M}}(\x,r))\Big)^{n}\right).
\end{eqnarray*}
Hence, for any arbitrary $\epsilon$ we can always find a $\delta(\epsilon,\mathcal{M},n)$ such that 
\begin{equation}
  P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{y}^{(t')}(\x) - y^{(t')}(\x)| \geq \epsilon \Big) \leq \delta(\epsilon,\mathcal{M},n). 
\end{equation}


Note that from Equation~\ref{eq: notnearrnn}, we can observe that $\lim_{n\to\infty}$ $1NN^{\mathcal{S}_n}_{\mathcal{M}}(\x,t) \to \x$; this implies \textit{asymptotic convergence of nearest neighbor}. We can also do a similar analysis for $KNN$ for any finite and fixed $K>0$, however for the sake of simplicity we have shown the finite sample bounds for $1NN$. In contract to previous works on nearest neighbor methods, the result shown Lemma~\ref{lm: smoothy} holds for any smooth distance metric, not just for a predefined distance metric.

%-------------------------------------------------

\begin{lemma}
\label{lm: ytotau}
If we can estimate the counterfactual %$\hat{y}^{(t)}(\cdot)$ 
using a finite sample $\mathcal{S}_n\overset{i.i.d}{\sim}\mu(\mathcal{Z}^n)$ such that the true counterfactual $y^{(t)}$ and the estimated counterfactual $\hat{y}^{(t)}(\cdot)$ are farther than $\epsilon'$ with probability less than $\delta'(\epsilon',\cdot,n)$ for any given $z\in\mathcal{Z}$ and $t \in \mathcal{T}$,
then the estimated individualized treatment $\hat{\tau}(\cdot)$ using a finite sample $\mathcal{S}_n\overset{i.i.d}{\sim}\mu(\mathcal{Z}^n)$ and the true individualized treatment effect $\tau(\cdot)$ are farther than $\epsilon$ with probability less than $\delta'(\frac{\epsilon}{2},\cdot,n)$.
$$ \forall t\in \mathcal{T}, ~P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{y}^{(t)}(\x) - y^{(t)}(\x)| \geq \epsilon' \Big) \leq \delta'(\epsilon',\cdot,n) \implies P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}\Big( |\hat{\tau}(\x) - \tau(\x)| \geq \epsilon \Big) \leq \delta'\Big(\frac{\epsilon}{2},\cdot,n\Big). $$
\end{lemma}
\textbf{Proof (Lemma~\ref{lm: ytotau})}.
Given that for any $\epsilon'>$, we can find an $ \delta^{'}(\epsilon',\cdot,n)$ such that 
\begin{equation*}
    \forall z\in\mathcal{Z},~\forall t'\in \mathcal{T},~P_{\mathcal{S}_n \sim \mu(\mathcal{Z}^n)}(|\hat{y}^{(t')}(\x) - y^{(t')}(\x)|\geq \epsilon^{'})\leq \delta^{'}_{\epsilon^{'}}(\epsilon',\cdot,n).
\end{equation*}
We can further deduce that
\begin{equation}
    \label{eq: andeq}
    P\Bigg( \bigvee_{t'\in\mathcal{T}}~\Big(|\hat{y}^{(t')}(\x) - y^{(t')}(\x)| \geq \epsilon^{'}\Big) \Bigg) \leq |\mathcal{T}|~\delta^{'}(\epsilon',\cdot,n).
\end{equation}
By the triangle inequality, we also know that
\begin{equation}
\label{eq: treq1}
  \sum_{t'\in\mathcal{T}}\Big|\hat{y}^{(t')}(\x) - y^{(t')}(\x)\Big|
    \geq \Bigg| \sum_{t'\in\mathcal{T}}\Big(\hat{y}^{(t')}(\x) - y^{(t')}(\x)\Big) \Bigg|.
\end{equation}
Deducting from Equation~\ref{eq: andeq}, we have
\begin{equation*}
    P\Bigg( \sum_{t'\in\mathcal{T}}~\Big(|\hat{y}^{(t')}(\x) - y^{(t')}(\x)|\Big)  \geq |\mathcal{T}|\epsilon^{'}\Bigg) \leq |\mathcal{T}|~\delta^{'}(\epsilon',\cdot,n).
\end{equation*}
Applying the triangle inequality from Equation~\ref{eq: treq1},
\begin{equation*}
    P\Bigg( \Big| \sum_{t'\in\mathcal{T}}~\Big(\hat{y}^{(t')}(\x) - y^{(t')}(\x)\Big)\Big|  \geq |\mathcal{T}|\epsilon^{'}\Bigg) \leq |\mathcal{T}|~\delta^{'}(\epsilon',\cdot,n).
\end{equation*}
Considering the case where $\mathcal{T}=\{0,1\}$
\begin{equation*}
    P\Bigg( \Big| \hat{\tau}(\x) - \tau(\x)\Big|  \geq 2\epsilon^{'}\Bigg) \leq 2\delta^{'}(\epsilon',\cdot,n).
\end{equation*}
Hence, we can conclude that
\begin{equation*}
    P\Bigg( \Big| \hat{\tau}(\x) - \tau(\x)\Big|  \geq \epsilon\Bigg) \leq 2\delta^{'}\Big(\frac{\epsilon}{2},\cdot,n\Big).
\end{equation*}



