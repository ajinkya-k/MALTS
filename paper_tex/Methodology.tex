% !TEX root = main.tex
\section{Matching After Learning to Stretch (MALTS)}\label{sec:method}
%MALTS is an almost exact matching method for causal inference, designed to work for experimental and observational datasets. 
 %\subsection{Weighted Nearest Neighbors with a Learned Distance Metric}

MALTS performs weighted nearest neighbors matching, where the weights for the nearest neighbors can be learned by minimizing the following objective:
\small
\begin{eqnarray*}
   \mathbf{W} \in \textrm{arg}\min_{\widetilde{\mathbf{W}}} \left[ \sum_{i \in \mathcal{S}^{(T)}_{tr}} \left\|y_{i} - \sum_{s_l \in \mathcal{S}^{(T)}_{tr}, i\neq l} \widetilde{W}_{i,l} y_{l}\right\| \right]
    &+& \left[\sum_{i \in \mathcal{S}^{(C)}_{tr}}  \left\|y_{i} - \sum_{l \in \mathcal{S}^{(C)}_{tr}, i\neq l} \widetilde{W}_{i,l} y_{l}\right\| \right]+ \textrm{Reg}(\widetilde{W}),
\end{eqnarray*}
\normalsize
where $\textrm{Reg}(\cdot)$ is a regularization function. We let $\widetilde{W}_{i,l}$ be a function of $\dis_{\mathcal{M}}(\x_i,\x_l)$. For example, the $\widetilde{W}_{i,l}$ can encode whether $l$ belongs to $i$'s $K$-nearest neighbors. Alternatively they can encode soft $\textrm{KNN}$ weights where $\widetilde{W}_{i,l} \propto e^{-\dis_{\mathcal{M}}(\x_i,\x_l)}$. 

As a reminder of our notation, we consider distance metric $\dis_{\mathcal{M}}$ parameterized by a set of parameters $\mathcal{M}$.
We use Euclidean distances for continuous covariates, namely distances of the form $\|\mathcal{M} \mathbf{x}_a -\mathcal{M} \mathbf{x}_b\|_2$ where $\mathcal{M}$ encodes the orientation of the data. Usually, $\mathcal{M}$ is hard-coded rather than learned; an example in causal inference literature is the classical Mahalanobis distance ($\mathcal{M}$ is fixed as the inverse covariance matrix for the observed covariates). This approach has been demonstrated to perform well in settings where all covariates are observed and the inferential target is the average treatment effect \citep{stuart2010matching}. We are interested instead in individualized treatment effects, and just as the choice of Euclidean norm in Mahalanobis distance matching depends on the estimand of interest, the stretch metric needs to be amended for this new estimand. We propose learning the parameters of a distance metric, $\mathcal{M}$, directly from the observed data rather than setting it beforehand.
The parameters of distance metric $\mathcal{M}$ can be learned such that $\mathbf{W}$ minimizes the objective function on the training set. 


%We need to define ``approximate closeness" differently for discrete covariates. If we use the same distance metric for both discrete and continuous data, then units that are close in continuous space might be arbitrarily far in discrete space or vice versa (e.g., a choice of either Hamming distance or Euclidean distance would have this problem when used for both discrete and continuous covariates--Euclidean distance may not be meaningful for discrete covariates, whereas Hamming distance makes little sense for continuous covariates). Because of this, it is not natural to parameterize a single form of distance metric to encourage both exact matching on discrete data and almost-exact matching for continuous data. While Mahalanobis-distance-matching papers recommend converting unordered categorical variables to binary indicators \citep{stuart2010matching}, this approach does not scale and in fact can introduce %a toenail problem by including
%an overwhelming number of irrelevant covariates. Thus, mixed data poses a different set of challenges than either one alone, given the geometry of the space.

In our framework we can define ``approximate closeness" differently for discrete covariates if desired. For continuous covariates, MALTS uses Euclidean distance, which is also a reasonable metric to use for binary data  \citep[e.g., Mahalanobis-distance-matching papers recommend converting unordered categorical variables to binary indicators, see][]{stuart2010matching}; however, there are benefits to using other metrics, such as weighted Hamming distances, for comparison among sets of binary covariates. 
%If we use the same distance metric for both discrete and continuous data, then units that are close in continuous space might be arbitrarily far in discrete space or vice versa (e.g., a choice of either Hamming distance or Euclidean distance would have this problem when used for both discrete and continuous covariates--Euclidean distance may not be meaningful for discrete covariates, whereas Hamming distance makes little sense for continuous covariates). Because of this, it is not natural to parameterize a single form of distance metric to encourage both exact matching on discrete data and almost-exact matching for continuous data. While Mahalanobis-distance-matching papers recommend converting unordered categorical variables to binary indicators \citep{stuart2010matching}, this approach does not scale and in fact can introduce
%an overwhelming number of irrelevant covariates. Thus, mixed data poses a different set of challenges than either one alone, given the geometry of the space.
%
%To accommodate continuous and discrete covariates, 
%
To accommodate a combination of Euclidean and Hamming distances,
we parameterize our distance metric in terms of two components: one is a learned weighted Euclidean distance for continuous covariates while the other is a learned weighted Hamming distance for discrete covariates as in the FLAME and DAME algorithms \citep{wang2017flame, DiengEtAl2018}. These components are separately parameterized by matrices $\mathcal{M}_c$ and $\mathcal{M}_d$ respectively, $\mathcal{M} = \left[\mathcal{M}_c, \mathcal{M}_d\right]$. Let $a = (a_c,a_d)$ and $b = (b_c,b_d)$ be the covariates for two individuals split into continuous and discrete pairs respectively. The distance metric we propose is thus given by:
$$\distance_\mathcal{M}(a,b) = d_{\mathcal{M}_c}(a_c,b_c) + d_{\mathcal{M}_d}(a_d,b_d) \text{, where}$$
  $$d_{\mathcal{M}_c}(a_c,b_c) = \|\mathcal{M}_{c}a_c - \mathcal{M}_{c}b_c\|_{2}, \hspace{0.25 cm}d_{\mathcal{M}_d}(a_d,b_d) = \sum_{j=0}^{|a_d|} \mathcal{M}_d^{(j,j)} \mathbbm{1}[a_d^{(j)}\neq b_d^{(j)}], $$
and $\mathbbm{1}[A]$ is the indicator that event $A$ occurred. 
We thus perform learned Hamming distance matching on the discrete covariates and learned-Mahalanobis-distance matching for continuous covariates. 

MALTS performs an honest causal inference by splitting the observed sample dataset $\mathcal{S}_n$ into a training set $\mathcal{S}_{tr}$ (not for matching) and an estimation set $\mathcal{S}_{est}$.
We learn $\mathcal{M}(\mathcal{S}_{tr})$ using the training sample $\mathcal{S}_{tr}$ such that 
\begin{equation}
    \mathcal{M}(\mathcal{S}_{tr}) \in \argmin_{\mathcal{M}} \left( c\|\mathcal{M}\|_\mathcal{F} + \Delta^{(C)}_{\mathcal{S}_{tr}}(\mathcal{M}) + \Delta^{(T)}_{\mathcal{S}_{tr}}(\mathcal{M}) \right)
\end{equation}
where,
$\|\cdot\|_{\mathcal{F}}$ is Frobenius norm of the matrix,
\begin{equation}
\begin{split}
    \Delta^{(C)}_{\mathcal{S}_{tr}}(\mathcal{M}) :&= \sum_{s_i\in\mathcal{S}^{(C)}_{tr}}\left| y_i - \sum_{s_l \in \mathcal{S}^{(C)}_{tr}} \frac{e^{-\dis_\mathcal{M}(\x_i,\x_l)}}{\sum_{s_k \in \mathcal{S}^{(C)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}y_l  \right| \\ &= \sum_{s_i\in\mathcal{S}^{(C)}_{tr}}\left|\sum_{s_l \in \mathcal{S}^{(C)}_{tr}} \frac{e^{-\dis_\mathcal{M}(\x_i,\x_l)}}{\sum_{s_k \in \mathcal{S}^{(C)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}(y_i-y_l)  \right|
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    \Delta^{(T)}_{\mathcal{S}_{tr}}(\mathcal{M}) :&= \sum_{s_i\in\mathcal{S}^{(T)}_{tr}}\left| y_i - \sum_{s_l \in \mathcal{S}^{(T)}_{tr}} \frac{e^{-\dis_\mathcal{M}(\x_i,\x_l)}}{\sum_{s_k \in \mathcal{S}^{(T)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}y_l  \right| \\ &= \sum_{s_i\in\mathcal{S}^{(T)}_{tr}}\left|\sum_{s_l \in \mathcal{S}^{(T)}_{tr}} \frac{e^{-\dis_\mathcal{M}(\x_i,\x_l)}}{\sum_{s_k \in \mathcal{S}^{(T)}_{tr}} e^{-\dis_\mathcal{M}(\x_i,\x_k)}}(y_i-y_l)\right|. 
\end{split}
\end{equation}

% we have approximately solved:
% \small
% \begin{eqnarray*}
% \mathcal{M} \in \textrm{arg}\min_{\mathcal{M}}\left[
% \sum_{i\in \textrm{treatment group}}
% \left( y_i^{\tr,T} - \frac{1}{K}\sum_{k \in \KNN_{\LL}(T,\x_i)} y_k^{\tr,T}
% \right)^2 
% + \sum_{i\in \textrm{control group}}
% \left( y_i^{\tr,C} - \frac{1}{K}\sum_{k \in \KNN_{\LL}(C,\x_i)}y_k^{\tr,C}
% \right)^2\right],  
% \end{eqnarray*}
% \normalsize
% where $\KNN_\LL(B,\x_i)$ is defined as the set of $K$ nearest points to $\x_i$ using $\textrm{distance}(\x_i,\x_k)$ parameterized by $\LL$. 

% For interpretability, we let $\mathcal{M}_c$ be a diagonal matrix, which allows stretches of the continuous covariates. This way, the magnitude of an entry in $\mathcal{M}_c$ or $\mathcal{M}_d$ provides the relative importance of the indicated covariate for the causal inference problem. 
% We use python scipy library's implementation of COBYLA, a non-gradient optimization method, to learn $\mathcal{M}$  \citep{scipy,Powell1994}.

We use the learned distance metric $\mathcal{M}(\mathcal{S}_{tr})$ to estimate conditional average treatment effects (CATEs) for each unit in the estimation set, using its nearest neighbors from the same estimation set. For any given unit $s$ in the estimation set, we construct a K-nearest neighbor matched group $\MG(s,\dis_{\mathcal{M}(\mathcal{S}_{tr})},\mathcal{S}_{est},K)$ using control set $\mathcal{S}^{(C)}_{est}$ and treatment set $\mathcal{S}^{(T)}_{est}$. For a choice of estimator $\phi$, the estimated CATE for a treated unit $s = (\x_s,y_s,t_s=T)$ is calculated as follows: 
% $\textrm{KNN}^{\mathcal{S}^{(C)}_{est}}_{\mathcal{M}(\mathcal{S}_{tr})}$ for treatment and control:
$$\hat{\tau}(\x) = y_s - \phi
\left(\MG(s,\dis_{\mathcal{M}(\mathcal{S}_{tr})},\mathcal{S}_{est},K)\right).$$

A simple example of $\phi$ is the empirical mean, i.e. $$\phi\left(\MG(s,\dis_{\mathcal{M}},\mathcal{S}_n,K)\right)=\frac{1}{K}\sum_{k\in\MG(s,\dis_{\mathcal{M}},\mathcal{S}_n,K)}y_k.$$ However, one can choose the estimator to be a weighted mean, linear regression or a non-parametric model like Random Forest.

For $\phi\left(\MG(s,\dis_{\mathcal{M}},\mathcal{S}_n,K)\right) = \sum_{k\in\MG(s,\dis_{\mathcal{M}},\mathcal{S}_n,K)} \widetilde{W}_{k}y_k$, if $\widetilde{W}_{k}$ is chosen to be proportional to $e^{\dis_{\mathcal{M}}(\x,\x_k)}$, then it leads to multi-robust and generalizable CATE estimates via soft KNN (as shown in Theorem \ref{th: robust} and Theorem \ref{th: gen} below), while letting $\widetilde{W}_k$ be proportional to $\mathbbm{1}\left[s_k\in\textrm{KNN}^{\mathcal{S}^{(C)}_{est}}_{\mathcal{M}(\mathcal{S}_{tr})}\right]$ produces reliable CATE estimates and interpretable matched groups.



\input{Theorem.tex}